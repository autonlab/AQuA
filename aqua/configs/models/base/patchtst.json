{
    "batch_size": 32,
    "lr": 0.01,
    "lr_drops": [],    
    "patch_len": [8, 16],
    "stride": [8, 16],
    "d_model": [128, 512],
    "d_ff": [256],
    "n_heads": 8,
    "dropout": 0.1,
    "e_layers": [2, 3],
    "activation": "gelu",
    "verbose": false,
    "output_attention": false
}

