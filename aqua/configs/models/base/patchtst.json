{
    "epochs": 15,
    "batch_size": 32,
    "lr": 0.01,
    "lr_drops": [],    
    "patch_len": 16,
    "stride": 8,
    "d_model": 512,
    "d_ff": 256,
    "n_heads": 8,
    "dropout": 0.1,
    "e_layers": 2,
    "activation": "gelu",
    "verbose": false,
    "output_attention": false
}

